input {
  kafka {
    topics => ["bro-raw"]
    add_field => { "[@metadata][stage]" => "broraw_kafka" }
    # Set this to one per kafka partition to scale up
    #consumer_threads => 4
    group_id => "bro_logstash"
    bootstrap_servers => "127.0.0.1:9092"
    codec => json
    auto_offset_reset => "earliest"
  }
}

filter {
  if "_jsonparsefailure" in [tags] {
    drop { }
  }

  if [@metadata][stage] == "broraw_kafka" {

    # Set the timestamp
    date { match => [ "ts", "ISO8601" ] }
    date { match => [ "ts", "UNIX" ] }

    # move metadata to new field
    mutate {
      rename => {
        "@stream" => "[@meta][stream]"
        "@system" => "[@meta][system]"
        "@proc"   => "[@meta][proc]"
      }
    }

    # Rename ID field from file analyzer logs
    if [@meta][stream] in ["pe", "x509", "files"] {
      mutate { rename => { "id" => "fuid" } }
      mutate {
        add_field => { "[@meta][event_type]" => "file" }
        add_field => { "[@meta][id]" => "%{fuid}" }
      }
    } else if [@meta][stream] in ["intel", "notice", "notice_alarm", "signatures", "traceroute"] {
      mutate { add_field => { "[@meta][event_type]" => "detection" } }
    } else if [@meta][stream] in [ "capture_loss", "cluster", "communication", "loaded_scripts", "packet_filter", "prof", "reporter", "stats", "stderr", "stdout" ] {
      mutate { add_field => { "[@meta][event_type]" => "diagnostic" } }
    } else if [@meta][stream] in ["netcontrol", "netcontrol_drop", "netcontrol_shunt", "netcontrol_catch_release", "openflow"] {
      mutate { add_field => { "[@meta][event_type]" => "netcontrol" } }
    } else if [@meta][stream] in ["known_certs", "known_devices", "known_hosts", "known_modbus", "known_services", "software"] {
      mutate { add_field => { "[@meta][event_type]" => "observations" } }
    } else if [@meta][stream] in ["barnyard2", "dpd", "unified2", "weird"] {
      mutate { add_field => { "[@meta][event_type]" => "miscellaneous" } }
    } else {

      # Network type
      mutate {
        convert => {
          "id_orig_p" => "integer"
          "id_resp_p" => "integer"
        }
        add_field => {
          "[@meta][event_type]" => "network"
          "[@meta][id]" => "%{uid}"
          "[@meta][orig_host]" => "%{id_orig_h}"
          "[@meta][orig_port]" => "%{id_orig_p}"
          "[@meta][resp_host]" => "%{id_resp_h}"
          "[@meta][resp_port]" => "%{id_resp_p}"
        }
      }
      geoip {
        source => "id_orig_h"
        target => "[@meta][geoip_orig]"
      }
      geoip {
        source => "id_resp_h"
        target => "[@meta][geoip_resp]"
      }
    }

    # Tie related records
    mutate { add_field => { "[@meta][related_ids]" => [] }}
    if [uid] {
      mutate { merge => {"[@meta][related_ids]" => "uid" }}
    }
    if [fuid] {
      mutate { merge => {"[@meta][related_ids]" => "fuid" }}
    }
    if [related_fuids] {
      mutate { merge => { "[@meta][related_ids]" => "related_fuids" }}
    }
    if [orig_fuids] {
      mutate { merge => { "[@meta][related_ids]" => "orig_fuids" }}
    }
    if [resp_fuids] {
      mutate { merge => { "[@meta][related_ids]" => "resp_fuids" }}
    }
    if [conn_uids] {
      mutate { merge => { "[@meta][related_ids]" => "conn_uids" }}
    }
    if [cert_chain_fuids] {
      mutate { merge => { "[@meta][related_ids]" => "cert_chain_fuids" }}
    }

    # Nest the entire document
    ruby {
      code => "
      require 'logstash/event'

      logtype = event.get('[@meta][stream]')
      ev_hash = event.to_hash
      meta_hash = ev_hash['@meta']
      timestamp = ev_hash['@timestamp']

      # Cleanup duplicate info
      #meta_hash.delete('stream')
      ev_hash.delete('@meta')
      ev_hash.delete('@timestamp')
      ev_hash.delete('tags')

      result = {
        logtype => ev_hash,
        '@meta' => meta_hash,
        '@timestamp' => timestamp
      }
      event.initialize( result )
      "
    }
    mutate { add_field => {"[@metadata][stage]" => "broraw_kafka" } }
    mutate{
      remove_field => [ "ts", "path", "@version", "host" ,"message"]
    }
  }
}

output {
  if [@metadata][stage] == "broraw_kafka" {
    kafka {
     codec => json
     topic_id => "bro-clean"
     bootstrap_servers => "127.0.0.1:9092"
    }

    elasticsearch {
      hosts => ["127.0.0.1"]
      index => "bro-%{+YYYY.MM.dd}"
      document_type => "%{[@meta][event_type]}"
      manage_template => false
    }
    stdout { codec => rubydebug }
  }
}
